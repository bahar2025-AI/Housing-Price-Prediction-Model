{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpZ0LUvu5BkdW517WqtgIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahar2025-AI/Housing-Price-Prediction-Model/blob/main/Housing_Price_Prediction_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0sgHjW5GguLk",
        "outputId": "99e7ec67-290e-40db-dc80-8871da0ae8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (20640, 8)\n",
            "Features: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
            "==================================================\n",
            "Enhanced Multivariate Regression Analysis\n",
            "==================================================\n",
            "Dataset shape: (20640, 8)\n",
            "Features: MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
            "\n",
            "1. Performing Exploratory Data Analysis\n",
            "--------------------------------------------------\n",
            "Summary Statistics:\n",
            "              count         mean          std         min         25%  \\\n",
            "MedInc      20640.0     3.870671     1.899822    0.499900    2.563400   \n",
            "HouseAge    20640.0    28.639486    12.585558    1.000000   18.000000   \n",
            "AveRooms    20640.0     5.429000     2.474173    0.846154    4.440716   \n",
            "AveBedrms   20640.0     1.096675     0.473911    0.333333    1.006079   \n",
            "Population  20640.0  1425.476744  1132.462122    3.000000  787.000000   \n",
            "AveOccup    20640.0     3.070655    10.386050    0.692308    2.429741   \n",
            "Latitude    20640.0    35.631861     2.135952   32.540000   33.930000   \n",
            "Longitude   20640.0  -119.569704     2.003532 -124.350000 -121.800000   \n",
            "MEDV        20640.0     2.068558     1.153956    0.149990    1.196000   \n",
            "\n",
            "                    50%          75%           max       skew  \n",
            "MedInc         3.534800     4.743250     15.000100   1.646657  \n",
            "HouseAge      29.000000    37.000000     52.000000   0.060331  \n",
            "AveRooms       5.229129     6.052381    141.909091  20.697869  \n",
            "AveBedrms      1.048780     1.099526     34.066667  31.316956  \n",
            "Population  1166.000000  1725.000000  35682.000000   4.935858  \n",
            "AveOccup       2.818116     3.282261   1243.333333  97.639561  \n",
            "Latitude      34.260000    37.710000     41.950000   0.465953  \n",
            "Longitude   -118.490000  -118.010000   -114.310000  -0.297801  \n",
            "MEDV           1.797000     2.647250      5.000010   0.977763  \n",
            "\n",
            "2. Performing Feature Engineering\n",
            "--------------------------------------------------\n",
            "Original features: 8\n",
            "Polynomial features: 44\n",
            "\n",
            "3. Training and Evaluating Models\n",
            "--------------------------------------------------\n",
            "\n",
            "Model Comparison:\n",
            "                    Name  Train RMSE  Test RMSE  Train R²  Test R²      MAE  CV R² (mean)  CV R² (std)  Training Time (s)\n",
            "       Linear Regression    0.719676   0.745581  0.612551 0.575788 0.533200      0.553031     0.061692           0.015666\n",
            "        Ridge Regression    0.719676   0.745554  0.612551 0.575819 0.533193      0.553038     0.061703           0.008573\n",
            "        Lasso Regression    0.819606   0.824349  0.497483 0.481420 0.622148      0.431008     0.073869           0.210602\n",
            "              ElasticNet    0.791935   0.797346  0.530841 0.514838 0.596967      0.456633     0.067848           0.008990\n",
            "Linear Regression (Poly)    0.648634   0.681397  0.685268 0.645682 0.467001    -25.129225    51.115275           0.079324\n",
            " Ridge Regression (Poly)    0.648637   0.680023  0.685265 0.647109 0.467021    -16.113771    33.103367           0.023758\n",
            " Lasso Regression (Poly)    0.816571   0.823487  0.501198 0.482504 0.619658      0.434319     0.072511           0.010267\n",
            "       ElasticNet (Poly)    0.790444   0.796859  0.532606 0.515430 0.595007      0.451984     0.061687           0.073694\n",
            "\n",
            "4. Hyperparameter Tuning\n",
            "--------------------------------------------------\n",
            "Ridge best parameters: {'alpha': 1.0}\n",
            "Ridge best score: 0.5193 (MSE)\n",
            "Lasso best parameters: {'alpha': 0.001}\n",
            "Lasso best score: 0.5192 (MSE)\n",
            "Elasticnet best parameters: {'alpha': 0.01, 'l1_ratio': 0.1}\n",
            "Elasticnet best score: 0.5214 (MSE)\n",
            "\n",
            "5. Residual Analysis\n",
            "--------------------------------------------------\n",
            "Training MSE: 0.5179\n",
            "Testing MSE: 0.5559\n",
            "Training R²: 0.6126\n",
            "Testing R²: 0.5758\n",
            "\n",
            "6. Feature Importance Analysis\n",
            "--------------------------------------------------\n",
            "\n",
            "Feature Importance:\n",
            "   Feature  Coefficient  Abs_Coefficient\n",
            "  Latitude    -0.899266         0.899266\n",
            " Longitude    -0.869916         0.869916\n",
            "    MedInc     0.829593         0.829593\n",
            " AveBedrms     0.305525         0.305525\n",
            "  AveRooms    -0.265397         0.265397\n",
            "  HouseAge     0.118817         0.118817\n",
            "  AveOccup    -0.039330         0.039330\n",
            "Population    -0.004480         0.004480\n",
            "Model saved as best_housing_model.pkl\n",
            "\n",
            "Analysis complete! All visualizations have been saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Enhanced Multivariate Regression Analysis\n",
        "# Based on original code from \"Mastering ML with Python in Six Steps\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as colors\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import warnings\n",
        "import time\n",
        "import joblib\n",
        "\n",
        "# Set aesthetic parameters for better visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Custom color palette\n",
        "colors_palette = sns.color_palette(\"viridis\", 8)\n",
        "sns.set_palette(colors_palette)\n",
        "\n",
        "# Function to load and prepare data\n",
        "def load_housing_data():\n",
        "    \"\"\"Load the Boston Housing dataset\"\"\"\n",
        "    from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "    # Load California Housing dataset (a more modern alternative to Boston Housing)\n",
        "    housing = fetch_california_housing()\n",
        "\n",
        "    # Create a DataFrame\n",
        "    features = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "    target = pd.Series(housing.target, name=\"MEDV\")\n",
        "\n",
        "    print(f\"Dataset shape: {features.shape}\")\n",
        "    print(f\"Features: {', '.join(features.columns)}\")\n",
        "\n",
        "    return features, target\n",
        "\n",
        "# Load data\n",
        "X, y = load_housing_data()\n",
        "\n",
        "# Exploratory Data Analysis\n",
        "def perform_eda(X, y):\n",
        "    \"\"\"Perform enhanced exploratory data analysis with visualizations\"\"\"\n",
        "    # Combining features and target for analysis\n",
        "    data = pd.concat([X, y], axis=1)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"Summary Statistics:\")\n",
        "    summary = data.describe().T\n",
        "    summary[\"skew\"] = data.skew()\n",
        "    print(summary)\n",
        "\n",
        "    # Create figure for histograms\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, column in enumerate(data.columns):\n",
        "        plt.subplot(3, 4, i+1)\n",
        "        sns.histplot(data[column], kde=True, color=colors_palette[i % len(colors_palette)])\n",
        "        plt.title(f'Distribution of {column}')\n",
        "        plt.tight_layout()\n",
        "    plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Correlation matrix with enhanced heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    corr = data.corr()\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
        "                square=True, linewidths=.5, annot=True, fmt=\".2f\", cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Correlation Matrix', fontsize=16)\n",
        "    plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Pairplot for key features (select a subset to avoid overcrowding)\n",
        "    # Find top correlated features with target\n",
        "    corr_with_target = abs(corr[y.name]).sort_values(ascending=False)\n",
        "    top_features = corr_with_target[1:5].index.tolist()  # Exclude target itself\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    subset_data = data[[y.name] + top_features]\n",
        "    sns.pairplot(subset_data, height=2.5, diag_kind='kde',\n",
        "                 plot_kws={'alpha': 0.6, 's': 80, 'edgecolor': 'k', 'linewidth': 0.5})\n",
        "    plt.savefig('pairplot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # 3D plot of top 2 features with target\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    x_feat, y_feat = top_features[0], top_features[1]\n",
        "\n",
        "    scatter = ax.scatter(data[x_feat],\n",
        "                         data[y_feat],\n",
        "                         data[y.name],\n",
        "                         c=data[y.name],\n",
        "                         cmap='viridis',\n",
        "                         s=50,\n",
        "                         alpha=0.7,\n",
        "                         edgecolor='w',\n",
        "                         linewidth=0.5)\n",
        "\n",
        "    ax.set_xlabel(x_feat, fontsize=12)\n",
        "    ax.set_ylabel(y_feat, fontsize=12)\n",
        "    ax.set_zlabel(y.name, fontsize=12)\n",
        "    ax.set_title(f'3D Relationship between {x_feat}, {y_feat} and {y.name}', fontsize=14)\n",
        "    plt.colorbar(scatter, ax=ax, shrink=0.6, aspect=20, label=y.name)\n",
        "\n",
        "    # Adjust the view angle for better visualization\n",
        "    ax.view_init(elev=30, azim=45)\n",
        "    plt.savefig('3d_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "# Feature Engineering\n",
        "def perform_feature_engineering(X, y):\n",
        "    \"\"\"Enhanced feature engineering with visualization\"\"\"\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Create polynomial features\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    X_poly = poly.fit_transform(X_scaled)\n",
        "\n",
        "    # Get feature names for polynomials\n",
        "    poly_features = poly.get_feature_names_out(input_features=X.columns)\n",
        "    X_poly_df = pd.DataFrame(X_poly, columns=poly_features)\n",
        "\n",
        "    print(f\"Original features: {X.shape[1]}\")\n",
        "    print(f\"Polynomial features: {X_poly.shape[1]}\")\n",
        "\n",
        "    # PCA for visualization and dimensionality reduction\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Visualize PCA results\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis',\n",
        "               alpha=0.7, s=70, edgecolor='w', linewidth=0.5)\n",
        "    plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    plt.title('PCA of Housing Features')\n",
        "    plt.colorbar(scatter, label='Housing Price')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig('pca_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Return both original scaled and polynomial features\n",
        "    return X_scaled, X_poly, pca\n",
        "\n",
        "# Model Training and Evaluation\n",
        "def train_and_evaluate_models(X, y, X_poly=None):\n",
        "    \"\"\"Train multiple regression models with enhanced evaluation\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    if X_poly is not None:\n",
        "        X_poly_train, X_poly_test, _, _ = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Models to evaluate\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Ridge Regression': Ridge(alpha=1.0),\n",
        "        'Lasso Regression': Lasso(alpha=0.1),\n",
        "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Standard features models\n",
        "    for name, model in models.items():\n",
        "        start_time = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        # Predictions\n",
        "        y_pred_train = model.predict(X_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "\n",
        "        # Evaluation metrics\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "        train_r2 = r2_score(y_train, y_pred_train)\n",
        "        test_r2 = r2_score(y_test, y_pred_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "        # Cross-validation\n",
        "        cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "        results.append({\n",
        "            'Name': f\"{name}\",\n",
        "            'Train RMSE': train_rmse,\n",
        "            'Test RMSE': test_rmse,\n",
        "            'Train R²': train_r2,\n",
        "            'Test R²': test_r2,\n",
        "            'MAE': mae,\n",
        "            'CV R² (mean)': cv_scores.mean(),\n",
        "            'CV R² (std)': cv_scores.std(),\n",
        "            'Training Time (s)': train_time\n",
        "        })\n",
        "\n",
        "    # Polynomial features models\n",
        "    if X_poly is not None:\n",
        "        for name, model in models.items():\n",
        "            start_time = time.time()\n",
        "            model.fit(X_poly_train, y_train)\n",
        "            train_time = time.time() - start_time\n",
        "\n",
        "            # Predictions\n",
        "            y_pred_train = model.predict(X_poly_train)\n",
        "            y_pred_test = model.predict(X_poly_test)\n",
        "\n",
        "            # Evaluation metrics\n",
        "            train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "            train_r2 = r2_score(y_train, y_pred_train)\n",
        "            test_r2 = r2_score(y_test, y_pred_test)\n",
        "            mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "            # Cross-validation\n",
        "            cv_scores = cross_val_score(model, X_poly, y, cv=5, scoring='r2')\n",
        "\n",
        "            results.append({\n",
        "                'Name': f\"{name} (Poly)\",\n",
        "                'Train RMSE': train_rmse,\n",
        "                'Test RMSE': test_rmse,\n",
        "                'Train R²': train_r2,\n",
        "                'Test R²': test_r2,\n",
        "                'MAE': mae,\n",
        "                'CV R² (mean)': cv_scores.mean(),\n",
        "                'CV R² (std)': cv_scores.std(),\n",
        "                'Training Time (s)': train_time\n",
        "            })\n",
        "\n",
        "    # Convert results to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Visualize model comparison\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    # RMSE Comparison\n",
        "    plt.subplot(2, 2, 1)\n",
        "    barwidth = 0.35\n",
        "    x = np.arange(len(results_df['Name']))\n",
        "    plt.bar(x - barwidth/2, results_df['Train RMSE'], barwidth, label='Train RMSE')\n",
        "    plt.bar(x + barwidth/2, results_df['Test RMSE'], barwidth, label='Test RMSE')\n",
        "    plt.xticks(x, results_df['Name'], rotation=45, ha='right')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('RMSE Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # R² Comparison\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.bar(x - barwidth/2, results_df['Train R²'], barwidth, label='Train R²')\n",
        "    plt.bar(x + barwidth/2, results_df['Test R²'], barwidth, label='Test R²')\n",
        "    plt.xticks(x, results_df['Name'], rotation=45, ha='right')\n",
        "    plt.ylabel('R²')\n",
        "    plt.title('R² Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # MAE\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.bar(x, results_df['MAE'], color='green', alpha=0.7)\n",
        "    plt.xticks(x, results_df['Name'], rotation=45, ha='right')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.title('Mean Absolute Error')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Training Time\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.bar(x, results_df['Training Time (s)'], color='purple', alpha=0.7)\n",
        "    plt.xticks(x, results_df['Name'], rotation=45, ha='right')\n",
        "    plt.ylabel('Seconds')\n",
        "    plt.title('Training Time')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# Hyperparameter Tuning\n",
        "def tune_hyperparameters(X, y):\n",
        "    \"\"\"Perform hyperparameter tuning with visualization\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define parameter grids\n",
        "    param_grids = {\n",
        "        'ridge': {\n",
        "            'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "        },\n",
        "        'lasso': {\n",
        "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "        },\n",
        "        'elasticnet': {\n",
        "            'alpha': [0.01, 0.1, 1.0, 10.0],\n",
        "            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Models\n",
        "    models = {\n",
        "        'ridge': Ridge(),\n",
        "        'lasso': Lasso(),\n",
        "        'elasticnet': ElasticNet()\n",
        "    }\n",
        "\n",
        "    # Store best models\n",
        "    best_models = {}\n",
        "    cv_results = {}\n",
        "\n",
        "    # Tuning\n",
        "    for name, model in models.items():\n",
        "        grid = GridSearchCV(model, param_grids[name], cv=5, scoring='neg_mean_squared_error')\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        # Store best model\n",
        "        best_models[name] = grid.best_estimator_\n",
        "        cv_results[name] = grid.cv_results_\n",
        "\n",
        "        print(f\"{name.capitalize()} best parameters: {grid.best_params_}\")\n",
        "        print(f\"{name.capitalize()} best score: {-grid.best_score_:.4f} (MSE)\")\n",
        "\n",
        "    # Visualize hyperparameter tuning results\n",
        "    # Ridge alpha vs MSE\n",
        "    plt.figure(figsize=(16, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    alphas = param_grids['ridge']['alpha']\n",
        "    mean_scores = -cv_results['ridge']['mean_test_score']\n",
        "    std_scores = cv_results['ridge']['std_test_score']\n",
        "\n",
        "    plt.semilogx(alphas, mean_scores, marker='o', linestyle='-', color=colors_palette[0])\n",
        "    plt.fill_between(alphas, mean_scores - std_scores, mean_scores + std_scores, alpha=0.3, color=colors_palette[0])\n",
        "    plt.xlabel('Alpha')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.title('Ridge Regression: Alpha vs MSE')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Lasso alpha vs MSE\n",
        "    plt.subplot(1, 3, 2)\n",
        "    alphas = param_grids['lasso']['alpha']\n",
        "    mean_scores = -cv_results['lasso']['mean_test_score']\n",
        "    std_scores = cv_results['lasso']['std_test_score']\n",
        "\n",
        "    plt.semilogx(alphas, mean_scores, marker='o', linestyle='-', color=colors_palette[1])\n",
        "    plt.fill_between(alphas, mean_scores - std_scores, mean_scores + std_scores, alpha=0.3, color=colors_palette[1])\n",
        "    plt.xlabel('Alpha')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.title('Lasso Regression: Alpha vs MSE')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # ElasticNet contour plot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    results = pd.DataFrame(cv_results['elasticnet'])\n",
        "    alphas = np.array(param_grids['elasticnet']['alpha'])\n",
        "    l1_ratios = np.array(param_grids['elasticnet']['l1_ratio'])\n",
        "\n",
        "    # Create a grid of alpha and l1_ratio values\n",
        "    alpha_grid, l1_ratio_grid = np.meshgrid(alphas, l1_ratios)\n",
        "\n",
        "    # Reshape the scores for contour plot\n",
        "    scores = np.zeros_like(alpha_grid)\n",
        "    for i, alpha in enumerate(alphas):\n",
        "        for j, l1_ratio in enumerate(l1_ratios):\n",
        "            mask = (results['param_alpha'] == alpha) & (results['param_l1_ratio'] == l1_ratio)\n",
        "            scores[j, i] = -results.loc[mask, 'mean_test_score'].values[0]\n",
        "\n",
        "    # Create contour plot\n",
        "    contour = plt.contourf(np.log10(alpha_grid), l1_ratio_grid, scores, levels=20, cmap='viridis', alpha=0.8)\n",
        "    plt.colorbar(contour, label='MSE')\n",
        "    plt.scatter(np.log10(alpha_grid), l1_ratio_grid, color='white', s=30, alpha=0.5)\n",
        "    plt.xlabel('log10(Alpha)')\n",
        "    plt.ylabel('L1 Ratio')\n",
        "    plt.title('ElasticNet: Alpha vs L1 Ratio')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hyperparameter_tuning.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return best_models\n",
        "\n",
        "# Analyze residuals\n",
        "def analyze_residuals(X, y, model):\n",
        "    \"\"\"Analyze and visualize residuals\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "\n",
        "    # Calculate residuals\n",
        "    residuals_train = y_train - y_pred_train\n",
        "    residuals_test = y_test - y_pred_test\n",
        "\n",
        "    # Create figure for residuals analysis\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Residuals vs Predicted (Training)\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.scatter(y_pred_train, residuals_train, alpha=0.6, color=colors_palette[0], edgecolor='w', linewidth=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residuals vs Predicted (Training)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Residuals vs Predicted (Testing)\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.scatter(y_pred_test, residuals_test, alpha=0.6, color=colors_palette[1], edgecolor='w', linewidth=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Residuals vs Predicted (Testing)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Residuals histogram (Training)\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.histplot(residuals_train, kde=True, color=colors_palette[0])\n",
        "    plt.xlabel('Residuals')\n",
        "    plt.title('Residuals Distribution (Training)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    # Residuals histogram (Testing)\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.histplot(residuals_test, kde=True, color=colors_palette[1])\n",
        "    plt.xlabel('Residuals')\n",
        "    plt.title('Residuals Distribution (Testing)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # QQ plot for normality check\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    from scipy import stats\n",
        "    stats.probplot(residuals_train, dist=\"norm\", plot=plt)\n",
        "    plt.title('Q-Q Plot (Training Residuals)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats.probplot(residuals_test, dist=\"norm\", plot=plt)\n",
        "    plt.title('Q-Q Plot (Testing Residuals)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('residuals_qq_plot.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "    r2_train = r2_score(y_train, y_pred_train)\n",
        "    r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"Training MSE: {mse_train:.4f}\")\n",
        "    print(f\"Testing MSE: {mse_test:.4f}\")\n",
        "    print(f\"Training R²: {r2_train:.4f}\")\n",
        "    print(f\"Testing R²: {r2_test:.4f}\")\n",
        "\n",
        "    # Save predictions vs actual\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot for test set\n",
        "    plt.scatter(y_test, y_pred_test, alpha=0.6, color=colors_palette[0], edgecolor='w', linewidth=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Actual vs Predicted Values (Test Set)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Feature Importance\n",
        "def analyze_feature_importance(X, model):\n",
        "    \"\"\"Analyze and visualize feature importance\"\"\"\n",
        "    # Get feature names\n",
        "    feature_names = X.columns\n",
        "\n",
        "    # Fit model\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Get coefficients\n",
        "    if hasattr(model, 'coef_'):\n",
        "        coefficients = model.coef_\n",
        "    else:\n",
        "        print(\"Model doesn't have coefficients attribute.\")\n",
        "        return\n",
        "\n",
        "    # Create DataFrame\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefficients\n",
        "    })\n",
        "\n",
        "    # Sort by absolute value\n",
        "    feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
        "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Bar plot of coefficients\n",
        "    colors = [colors_palette[0] if c > 0 else colors_palette[1] for c in feature_importance['Coefficient']]\n",
        "    plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors)\n",
        "    plt.xlabel('Coefficient')\n",
        "    plt.title('Feature Importance (Coefficients)')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return feature_importance\n",
        "\n",
        "# Save model\n",
        "def save_best_model(model, filename='best_model.pkl'):\n",
        "    \"\"\"Save the best model to disk\"\"\"\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"Model saved as {filename}\")\n",
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    \"\"\"Main execution flow\"\"\"\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enhanced Multivariate Regression Analysis\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Load data\n",
        "    X, y = load_housing_data()\n",
        "\n",
        "    # Exploratory Data Analysis\n",
        "    print(\"\\n1. Performing Exploratory Data Analysis\")\n",
        "    print(\"-\"*50)\n",
        "    data = perform_eda(X, y)\n",
        "\n",
        "    # Feature Engineering\n",
        "    print(\"\\n2. Performing Feature Engineering\")\n",
        "    print(\"-\"*50)\n",
        "    X_scaled, X_poly, pca = perform_feature_engineering(X, y)\n",
        "\n",
        "    # Model Training and Evaluation\n",
        "    print(\"\\n3. Training and Evaluating Models\")\n",
        "    print(\"-\"*50)\n",
        "    results = train_and_evaluate_models(X_scaled, y, X_poly)\n",
        "    print(\"\\nModel Comparison:\")\n",
        "    print(results.to_string(index=False))\n",
        "\n",
        "    # Hyperparameter Tuning\n",
        "    print(\"\\n4. Hyperparameter Tuning\")\n",
        "    print(\"-\"*50)\n",
        "    best_models = tune_hyperparameters(X_scaled, y)\n",
        "\n",
        "    # Select best model (Ridge in this case for demonstration)\n",
        "    best_model = best_models['ridge']\n",
        "\n",
        "    # Analyze Residuals\n",
        "    print(\"\\n5. Residual Analysis\")\n",
        "    print(\"-\"*50)\n",
        "    analyze_residuals(X_scaled, y, best_model)\n",
        "\n",
        "    # Feature Importance\n",
        "    print(\"\\n6. Feature Importance Analysis\")\n",
        "    print(\"-\"*50)\n",
        "    X_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "    feature_importance = analyze_feature_importance(X_df, best_model)\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance.to_string(index=False))\n",
        "\n",
        "    # Save best model\n",
        "    save_best_model(best_model, 'best_housing_model.pkl')\n",
        "\n",
        "    print(\"\\nAnalysis complete! All visualizations have been saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}